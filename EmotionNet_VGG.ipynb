{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionNet_VGG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eTEfEe2bjm19",
        "beI2AzYwpyUs",
        "E8Td8N0Ay65q",
        "_1gUIMKYImrh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitul01/FER/blob/main/EmotionNet_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neDbJ9a6f4L6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2937fe7b-569c-413d-dc02-58391cfbfc61"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oqqFQm6gCyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea0ae0d-96d4-40ef-e3b6-bec7b8c251ec"
      },
      "source": [
        "cd /content/drive/MyDrive/colab-20210303T131708Z-001/colab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-20210303T131708Z-001/colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_hW-6FcUh4a"
      },
      "source": [
        "# EmotionNet VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRulxXUOgNcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836d0f7a-09ff-405b-af2c-233bb51b58d4"
      },
      "source": [
        "# set matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import packages\n",
        "from config import emotion_config as config\n",
        "from pipeline.preprocessing import ImageToArrayPreprocessor\n",
        "from pipeline.callbacks import EpochCheckpoint\n",
        "from pipeline.callbacks import TrainingMonitor\n",
        "from pipeline.io import HDF5DatasetGenerator\n",
        "from pipeline.nn.conv import EmotionVGGNet\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# construct the training and testing image generators for data\n",
        "# augmentation, then initialize the image preprocessor\n",
        "trainAug = ImageDataGenerator(rotation_range = 10, zoom_range = 0.1,\n",
        "    horizontal_flip = True, rescale = 1 / 255.0, fill_mode = \"nearest\")\n",
        "\n",
        "valAug = ImageDataGenerator(rescale = 1 / 255.0)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "\n",
        "trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, config.BATCH_SIZE,\n",
        "    aug = trainAug, preprocessors = [iap], classes = config.NUM_CLASSES)\n",
        "valGen = HDF5DatasetGenerator(config.VAL_HDF5, config.BATCH_SIZE,\n",
        "    aug = valAug, preprocessors = [iap], classes = config.NUM_CLASSES)\n",
        "\n",
        "# if there is no specific model checkpoint supplied, then initialize\n",
        "# the network and compile the model\n",
        "#if args[\"model\"] is None:\n",
        "print(\"compiling model...\")\n",
        "model = EmotionVGGNet.build(width = 48, height = 48, depth = 1,\n",
        "    classes = config.NUM_CLASSES)\n",
        "# opt = SGD(lr = 1e-2, momentum = 0.9, nesterov = True)\n",
        "opt = Adam(lr = 1e-3)\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = opt,\n",
        "    metrics = [\"accuracy\"])\n",
        "\n",
        "# otherwise, load the checkpoint from disk\n",
        "#else:\n",
        "#    print(\"[INFO] loding {}...\".format(args[\"model\"]))\n",
        "#    model = load_model(args[\"model\"])\n",
        "\n",
        "# update the learning rate\n",
        "# print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "# K.set_value(model.optimizer.lr, 1e-5)\n",
        "# print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n",
        "\n",
        "# construct the set of callbacks\n",
        "figPath = os.path.sep.join([config.OUTPUT_PATH, \"vggnet_emotion_1.png\"])\n",
        "jsonPath = os.path.sep.join([config.OUTPUT_PATH, \"vggnet_emotion_1.json\"])\n",
        "\n",
        "callbacks = [\n",
        "    EpochCheckpoint(\"checkpoints\", every = 5, #startAt = args[\"start_epoch\"]\n",
        "                     ),TrainingMonitor(figPath, jsonPath = jsonPath) \n",
        "                     #startAt = args[\"start_epoch\"]och\"]                    \n",
        " ]\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=3)\n",
        "# early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "# checkpointer = ModelCheckpoint('drive/My Drive/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.00005,\n",
        "    patience=7,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.6,\n",
        "    patience=4,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping,\n",
        "    lr_scheduler,\n",
        "]\n",
        "\n",
        "# train network\n",
        "\n",
        "history=model.fit_generator(\n",
        "    trainGen.generator(),\n",
        "    steps_per_epoch = trainGen.numImages // config.BATCH_SIZE,\n",
        "    validation_data = valGen.generator(),\n",
        "    validation_steps = valGen.numImages // config.BATCH_SIZE,\n",
        "    epochs = 100,\n",
        "    max_queue_size = config.BATCH_SIZE * 2,\n",
        "    callbacks = callbacks,\n",
        "    verbose = 1\n",
        "    \n",
        ")\n",
        "\n",
        "# close the dataset\n",
        "\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-20210303T131708Z-001/colab/pipeline/io/hdf5datasetgenerator.py:20: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.db = h5py.File(dbPath)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "compiling model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "224/224 [==============================] - 48s 63ms/step - loss: 3.5553 - accuracy: 0.2071 - val_loss: 2.8527 - val_accuracy: 0.2955\n",
            "Epoch 2/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 2.7609 - accuracy: 0.3228 - val_loss: 2.3620 - val_accuracy: 0.4478\n",
            "Epoch 3/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 2.4031 - accuracy: 0.4013 - val_loss: 2.1451 - val_accuracy: 0.4791\n",
            "Epoch 4/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 2.1399 - accuracy: 0.4600 - val_loss: 1.8961 - val_accuracy: 0.5265\n",
            "Epoch 5/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.9514 - accuracy: 0.4919 - val_loss: 1.7162 - val_accuracy: 0.5550\n",
            "Epoch 6/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.8047 - accuracy: 0.5133 - val_loss: 1.6166 - val_accuracy: 0.5552\n",
            "Epoch 7/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.6908 - accuracy: 0.5291 - val_loss: 1.5236 - val_accuracy: 0.5706\n",
            "Epoch 8/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.6067 - accuracy: 0.5388 - val_loss: 1.4670 - val_accuracy: 0.5767\n",
            "Epoch 9/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.5431 - accuracy: 0.5546 - val_loss: 1.4366 - val_accuracy: 0.5893\n",
            "Epoch 10/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.4893 - accuracy: 0.5644 - val_loss: 1.4025 - val_accuracy: 0.5739\n",
            "Epoch 11/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.4531 - accuracy: 0.5746 - val_loss: 1.4259 - val_accuracy: 0.5770\n",
            "Epoch 12/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.4352 - accuracy: 0.5740 - val_loss: 1.4461 - val_accuracy: 0.5681\n",
            "Epoch 13/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.4030 - accuracy: 0.5850 - val_loss: 1.3642 - val_accuracy: 0.5968\n",
            "Epoch 14/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3861 - accuracy: 0.5893 - val_loss: 1.3436 - val_accuracy: 0.6119\n",
            "Epoch 15/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.3786 - accuracy: 0.5943 - val_loss: 1.3485 - val_accuracy: 0.5965\n",
            "Epoch 16/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3630 - accuracy: 0.6022 - val_loss: 1.3593 - val_accuracy: 0.6021\n",
            "Epoch 17/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3581 - accuracy: 0.5987 - val_loss: 1.3237 - val_accuracy: 0.6063\n",
            "Epoch 18/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.3362 - accuracy: 0.6099 - val_loss: 1.3324 - val_accuracy: 0.6105\n",
            "Epoch 19/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.3413 - accuracy: 0.6086 - val_loss: 1.3060 - val_accuracy: 0.6136\n",
            "Epoch 20/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.3263 - accuracy: 0.6159 - val_loss: 1.3040 - val_accuracy: 0.6138\n",
            "Epoch 21/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3329 - accuracy: 0.6178 - val_loss: 1.3679 - val_accuracy: 0.5912\n",
            "Epoch 22/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.3262 - accuracy: 0.6179 - val_loss: 1.2919 - val_accuracy: 0.6242\n",
            "Epoch 23/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3170 - accuracy: 0.6224 - val_loss: 1.2834 - val_accuracy: 0.6233\n",
            "Epoch 24/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.3192 - accuracy: 0.6234 - val_loss: 1.3093 - val_accuracy: 0.6217\n",
            "Epoch 25/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3135 - accuracy: 0.6227 - val_loss: 1.2846 - val_accuracy: 0.6336\n",
            "Epoch 26/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.3042 - accuracy: 0.6262 - val_loss: 1.2839 - val_accuracy: 0.6309\n",
            "Epoch 27/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.3053 - accuracy: 0.6305 - val_loss: 1.2875 - val_accuracy: 0.6197\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
            "Epoch 28/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.2669 - accuracy: 0.6446 - val_loss: 1.2274 - val_accuracy: 0.6429\n",
            "Epoch 29/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.2237 - accuracy: 0.6539 - val_loss: 1.2012 - val_accuracy: 0.6496\n",
            "Epoch 30/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.2081 - accuracy: 0.6570 - val_loss: 1.2173 - val_accuracy: 0.6392\n",
            "Epoch 31/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.1928 - accuracy: 0.6602 - val_loss: 1.2269 - val_accuracy: 0.6348\n",
            "Epoch 32/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.1943 - accuracy: 0.6594 - val_loss: 1.2315 - val_accuracy: 0.6348\n",
            "Epoch 33/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.1900 - accuracy: 0.6600 - val_loss: 1.1921 - val_accuracy: 0.6526\n",
            "Epoch 34/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.1829 - accuracy: 0.6649 - val_loss: 1.2412 - val_accuracy: 0.6336\n",
            "Epoch 35/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.1825 - accuracy: 0.6662 - val_loss: 1.2393 - val_accuracy: 0.6370\n",
            "Epoch 36/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.1719 - accuracy: 0.6700 - val_loss: 1.1925 - val_accuracy: 0.6588\n",
            "Epoch 37/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.1724 - accuracy: 0.6733 - val_loss: 1.2380 - val_accuracy: 0.6334\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
            "Epoch 38/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.1393 - accuracy: 0.6873 - val_loss: 1.1758 - val_accuracy: 0.6599\n",
            "Epoch 39/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.1030 - accuracy: 0.6970 - val_loss: 1.1550 - val_accuracy: 0.6655\n",
            "Epoch 40/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.0906 - accuracy: 0.6950 - val_loss: 1.1664 - val_accuracy: 0.6641\n",
            "Epoch 41/100\n",
            "224/224 [==============================] - 13s 57ms/step - loss: 1.0755 - accuracy: 0.7040 - val_loss: 1.1884 - val_accuracy: 0.6529\n",
            "Epoch 42/100\n",
            "224/224 [==============================] - 13s 56ms/step - loss: 1.0770 - accuracy: 0.7027 - val_loss: 1.1654 - val_accuracy: 0.6685\n",
            "Epoch 43/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 1.0622 - accuracy: 0.7070 - val_loss: 1.1704 - val_accuracy: 0.6699\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
            "Epoch 44/100\n",
            "224/224 [==============================] - 13s 59ms/step - loss: 1.0451 - accuracy: 0.7115 - val_loss: 1.1418 - val_accuracy: 0.6713\n",
            "Epoch 45/100\n",
            "224/224 [==============================] - 13s 59ms/step - loss: 1.0138 - accuracy: 0.7245 - val_loss: 1.1377 - val_accuracy: 0.6794\n",
            "Epoch 46/100\n",
            "224/224 [==============================] - 13s 59ms/step - loss: 1.0107 - accuracy: 0.7243 - val_loss: 1.1379 - val_accuracy: 0.6674\n",
            "Epoch 47/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 0.9840 - accuracy: 0.7374 - val_loss: 1.1432 - val_accuracy: 0.6747\n",
            "Epoch 48/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 0.9849 - accuracy: 0.7325 - val_loss: 1.1266 - val_accuracy: 0.6791\n",
            "Epoch 49/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 0.9742 - accuracy: 0.7354 - val_loss: 1.1553 - val_accuracy: 0.6696\n",
            "Epoch 50/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 0.9695 - accuracy: 0.7383 - val_loss: 1.1410 - val_accuracy: 0.6755\n",
            "Epoch 51/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 0.9520 - accuracy: 0.7428 - val_loss: 1.1494 - val_accuracy: 0.6744\n",
            "Epoch 52/100\n",
            "224/224 [==============================] - 13s 58ms/step - loss: 0.9565 - accuracy: 0.7462 - val_loss: 1.1522 - val_accuracy: 0.6775\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
            "Epoch 00052: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65zqwKGMSOh8",
        "outputId": "6b10e4ee-6a8f-45ec-a22b-373d7d0bb118"
      },
      "source": [
        "testAug = ImageDataGenerator(rescale = 1 / 255.0)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "testGen = HDF5DatasetGenerator(config.TEST_HDF5, config.BATCH_SIZE,\n",
        "    aug = testAug, preprocessors = [iap], classes = config.NUM_CLASSES)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-20210303T131708Z-001/colab/pipeline/io/hdf5datasetgenerator.py:20: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.db = h5py.File(dbPath)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0jdujekSOqf",
        "outputId": "f855ccad-cf94-4d3a-e433-07890a762303"
      },
      "source": [
        "predictions = model.predict_generator(testGen.generator(),steps=testGen.numImages //64, max_queue_size=64*2)\n",
        "\n",
        "(loss, acc) = model.evaluate_generator(\n",
        "\ttestGen.generator(),\n",
        "\tsteps=testGen.numImages // config.BATCH_SIZE,\n",
        "\tmax_queue_size=config.BATCH_SIZE * 2)\n",
        "print(\"[INFO] accuracy: {:.2f}\".format(acc * 100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] accuracy: 66.27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Td8N0Ay65q"
      },
      "source": [
        "#SeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tu4OfxW8Jyo",
        "outputId": "a9466079-ec48-46df-d462-084e87d3ae51"
      },
      "source": [
        "!pip install keras-vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-vggface in /usr/local/lib/python3.7/dist-packages (0.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (3.13)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (7.0.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (1.19.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (2.4.3)\n",
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhl9WXpkzXj-"
      },
      "source": [
        "from keras_vggface.vggface import VGGFace\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "%matplotlib inline\n",
        "from config import emotion_config as config\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras.utils import plot_model\n",
        "from sklearn.metrics import *\n",
        "from keras.engine import Model\n",
        "from keras.layers import Input, Flatten, Dense, Activation, Conv2D, MaxPool2D, BatchNormalization, Dropout, MaxPooling2D\n",
        "from pipeline.preprocessing import ImageToArrayPreprocessor\n",
        "from pipeline.io import HDF5DatasetGenerator\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.optimizers import Adam\n",
        "from pipeline.callbacks import EpochCheckpoint\n",
        "from pipeline.callbacks import TrainingMonitor\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37Y6KcJYzo9I",
        "outputId": "1f3bb893-32c7-4f0c-eeb4-db3cf43a5f27"
      },
      "source": [
        "trainAug = ImageDataGenerator(\n",
        "                            rescale=1./255,\n",
        "                            featurewise_center=False,\n",
        "                            featurewise_std_normalization=False,\n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            zoom_range=0.1,\n",
        "                            horizontal_flip=True)\n",
        "\n",
        "valAug= ImageDataGenerator(rescale=1./255,)\n",
        "\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, config.BATCH_SIZE,\n",
        "    aug = trainAug, preprocessors = [iap], classes = config.NUM_CLASSES,)\n",
        "\n",
        "valGen = HDF5DatasetGenerator(config.VAL_HDF5, config.BATCH_SIZE,\n",
        "    aug = valAug, preprocessors = [iap], classes = config.NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab-20210303T131708Z-001/colab/pipeline/io/hdf5datasetgenerator.py:20: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  self.db = h5py.File(dbPath)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRlkYpRFe2Lb"
      },
      "source": [
        "EPOCHS = 100\n",
        "BS = 128\n",
        "DROPOUT_RATE = 0.5\n",
        "FROZEN_LAYER_NUM = 201\n",
        "\n",
        "ADAM_LEARNING_RATE = 0.001\n",
        "SGD_LEARNING_RATE = 0.01\n",
        "SGD_DECAY = 0.0001\n",
        "\n",
        "Resize_pixelsize = 197\n",
        "vgg_notop = VGGFace(input_shape=(197,197,3),model='senet50',include_top=False)\n",
        "last_layer = vgg_notop.get_layer('avg_pool').output\n",
        "\n",
        "x = Flatten(name='flatten')(last_layer)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096, activation='relu', name='fc6')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu', name='fc7')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "batch_norm_indices = [2, 6, 9, 12, 21, 25, 28, 31, 42, 45, 48, 59, 62, 65, 74, 78, 81, 84, 95, 98, 101, 112, 115, 118, 129, 132, 135, 144, 148, 151, 154, 165, 168, 171, 182, 185, 188, 199, 202, 205, 216, 219, 222, 233, 236, 239, 248, 252, 255, 258, 269, 272, 275]    \n",
        "for i in range(FROZEN_LAYER_NUM):\n",
        "    if i not in batch_norm_indices:\n",
        "        vgg_notop.layers[i].trainable = False\n",
        "\n",
        "out = Dense(6, activation='softmax', name='classifier')(x)\n",
        "\n",
        "model = Model(vgg_notop.input, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "6ixVfiyvy8Lm",
        "outputId": "abd786f5-c91d-4369-9999-3fecf8763335"
      },
      "source": [
        "opt = Adam(lr = 1e-4)\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = opt,metrics = [\"accuracy\"])\n",
        "\n",
        "figPath = os.path.sep.join([config.OUTPUT_PATH, \"vggface_senet50.png\"])\n",
        "jsonPath = os.path.sep.join([config.OUTPUT_PATH, \"vggface_senet50.json\"])\n",
        "\n",
        "callbacks = [\n",
        "    EpochCheckpoint(\"checkpoints\", every = 5, #startAt = args[\"start_epoch\"]\n",
        "                     ),TrainingMonitor(figPath, jsonPath = jsonPath) \n",
        "                     #startAt = args[\"start_epoch\"]och\"]                    \n",
        " ]\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.00005,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping,\n",
        "    lr_scheduler,\n",
        "]\n",
        "\n",
        "# train network\n",
        "history=model.fit_generator(\n",
        "    trainGen.generator(),\n",
        "    steps_per_epoch = trainGen.numImages // config.BATCH_SIZE,\n",
        "    validation_data = valGen.generator(),\n",
        "    validation_steps = valGen.numImages // config.BATCH_SIZE,\n",
        "    epochs = 100,\n",
        "    max_queue_size = config.BATCH_SIZE * 2,\n",
        "    callbacks = callbacks,\n",
        "    verbose = 1\n",
        "    \n",
        ")\n",
        "\n",
        "# close the dataset\n",
        "\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d207ba3214e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmax_queue_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  input depth must be evenly divisible by filter depth: 1 vs 3\n\t [[node model_1/conv1/7x7_s2/Conv2D (defined at <ipython-input-29-d207ba3214e2>:42) ]] [Op:__inference_train_function_127190]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdYLLEevI9G7",
        "outputId": "09850eaa-869d-44bd-f50f-5213afeaa345"
      },
      "source": [
        "!pip install keras-vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-vggface\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/7d/5f0319ebdc09ac1a2272364fa9583f5067b6f8aff93fbbf8835d81cbaad7/keras_vggface-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from keras-vggface) (7.0.0)\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.6\n",
            "Collecting keras_applications\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.15.0)\n",
            "Installing collected packages: keras-applications\n",
            "Successfully installed keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8sIkaSfI3zU"
      },
      "source": [
        "#from keras_vggface.vggface import VGGFace\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.python.lib.io import file_io\n",
        "\n",
        "%matplotlib inline\n",
        "from config import emotion_config as config\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras_vggface.vggface import VGGFace\n",
        "from keras.utils import plot_model\n",
        "from sklearn.metrics import *\n",
        "from keras.engine import Model\n",
        "from keras.layers import Input, Flatten, Dense, Activation, Conv2D, MaxPool2D, BatchNormalization, Dropout, MaxPooling2D\n",
        "from pipeline.preprocessing import ImageToArrayPreprocessor\n",
        "from pipeline.io import HDF5DatasetGenerator\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.optimizers import Adam\n",
        "from pipeline.callbacks import EpochCheckpoint\n",
        "from pipeline.callbacks import TrainingMonitor\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8lk8cj3I0L2"
      },
      "source": [
        "trainAug = ImageDataGenerator(\n",
        "                            rescale=1./255,\n",
        "                            featurewise_center=False,\n",
        "                            featurewise_std_normalization=False,\n",
        "                            rotation_range=10,\n",
        "                            width_shift_range=0.1,\n",
        "                            height_shift_range=0.1,\n",
        "                            zoom_range=0.1,\n",
        "                            horizontal_flip=True)\n",
        "\n",
        "valAug= ImageDataGenerator(rescale=1./255,)\n",
        "\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, config.BATCH_SIZE,\n",
        "    aug = trainAug, preprocessors = [iap], classes = config.NUM_CLASSES,)\n",
        "\n",
        "valGen = HDF5DatasetGenerator(config.VAL_HDF5, config.BATCH_SIZE,\n",
        "    aug = valAug, preprocessors = [iap], classes = config.NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oB9-xF7InQH"
      },
      "source": [
        "model = VGGFace(classes = config.NUM_CLASSES,weights=None,input_shape=(48,48,1))\n",
        "\n",
        "opt = Adam(lr = 1e-4)\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = opt,metrics = [\"accuracy\"])\n",
        "\n",
        "figPath = os.path.sep.join([config.OUTPUT_PATH, \"vggface.png\"])\n",
        "jsonPath = os.path.sep.join([config.OUTPUT_PATH, \"vggface.json\"])\n",
        "\n",
        "callbacks = [\n",
        "    EpochCheckpoint(\"checkpoints\", every = 5, #startAt = args[\"start_epoch\"]\n",
        "                     ),TrainingMonitor(figPath, jsonPath = jsonPath) \n",
        "                     #startAt = args[\"start_epoch\"]och\"]                    \n",
        " ]\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    min_delta=0.00005,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping,\n",
        "    lr_scheduler,\n",
        "]\n",
        "\n",
        "# train network\n",
        "\n",
        "history=model.fit_generator(\n",
        "    trainGen.generator(),\n",
        "    steps_per_epoch = trainGen.numImages // config.BATCH_SIZE,\n",
        "    validation_data = valGen.generator(),\n",
        "    validation_steps = valGen.numImages // config.BATCH_SIZE,\n",
        "    epochs = 100,\n",
        "    max_queue_size = config.BATCH_SIZE * 2,\n",
        "    callbacks = callbacks,\n",
        "    verbose = 1\n",
        "    \n",
        ")\n",
        "\n",
        "# close the dataset\n",
        "\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D30PLgz2JMUl"
      },
      "source": [
        "model.save_weights('model_vggFace.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZUDJSlyRohD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNPbt_sRSM_W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}